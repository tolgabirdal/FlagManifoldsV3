{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10, MNIST\n",
    "from torchvision.models import alexnet\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from easyfsl.samplers import TaskSampler\n",
    "from easyfsl.utils import plot_images, sliding_average\n",
    "\n",
    "from AlexNetLastTwoLayers import AlexNetLastTwoLayers\n",
    "from PrototypicalNetworks import PrototypicalNetworks\n",
    "from PrototypicalFlagNetworks import PrototypicalFlagNetworks\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_accuracy(logits, label):\n",
    "    pred = torch.argmax(logits, dim=1).view(-1)\n",
    "    label = label.view(-1)\n",
    "    accuracy = 100 * pred.eq(label).float().mean()\n",
    "    return accuracy\n",
    "\n",
    "def one_hot(indices, depth):\n",
    "    \"\"\"\n",
    "    Returns a one-hot tensor.\n",
    "    This is a PyTorch equivalent of Tensorflow's tf.one_hot.\n",
    "        \n",
    "    Parameters:\n",
    "      indices:  a (n_batch, m) Tensor or (m) Tensor.\n",
    "      depth: a scalar. Represents the depth of the one hot dimension.\n",
    "    Returns: a (n_batch, m, depth) Tensor or (m, depth) Tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_indicies = torch.zeros(indices.size() + torch.Size([depth])).cuda()\n",
    "    index = indices.view(indices.size()+torch.Size([1]))\n",
    "    encoded_indicies = encoded_indicies.scatter_(1,index,1)\n",
    "    \n",
    "    return encoded_indicies\n",
    "\n",
    "def training_epoch(model_, data_loader, optimizer):\n",
    "    all_loss = []\n",
    "    model_.train()\n",
    "    with tqdm(\n",
    "        enumerate(data_loader), total=len(data_loader), desc=\"Training\"\n",
    "    ) as tqdm_train:\n",
    "        for episode_index, (\n",
    "            support_images,\n",
    "            support_labels,\n",
    "            query_images,\n",
    "            query_labels,\n",
    "            _,\n",
    "        ) in tqdm_train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logit_query = model_(support_images.cuda(), support_labels.cuda(), query_images.cuda())\n",
    "\n",
    "            train_way = len(torch.unique(support_labels))\n",
    "            smoothed_one_hot = one_hot(query_labels.reshape(-1).cuda(), train_way)\n",
    "            log_prb = F.log_softmax(logit_query.reshape(-1, train_way), dim=1)\n",
    "            loss = -(smoothed_one_hot * log_prb).sum(dim=1)\n",
    "            loss = loss.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            all_loss.append(loss.item())\n",
    "\n",
    "            tqdm_train.set_postfix(loss=mean(all_loss))\n",
    "\n",
    "    return mean(all_loss)\n",
    "\n",
    "def val_evaluate(model_, val_loader):\n",
    "    model_.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed during validation\n",
    "        for val_support_images, val_support_labels, val_query_images, val_query_labels, _ in val_loader:\n",
    "            # Obtain validation predictions\n",
    "            val_preds = model_(val_support_images.cuda(), val_support_labels.cuda(), val_query_images.cuda())\n",
    "            \n",
    "            # Count correct predictions\n",
    "            correct += (val_preds.argmax(dim=2).reshape(-1) == val_query_labels.cuda()).sum().item()\n",
    "            total += val_query_labels.size(0)\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = correct / total\n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "# np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "# random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))\n",
    "        ]\n",
    ")\n",
    "\n",
    "\n",
    "train_data = CIFAR10(\n",
    "    root=\"../data\",\n",
    "    transform= transform,\n",
    "    download=True,\n",
    "    train = True\n",
    ")\n",
    "test_data = CIFAR10(\n",
    "    root=\"../data\",\n",
    "    transform=transform,\n",
    "    download=True,\n",
    "    train = False   \n",
    ")\n",
    "\n",
    "\n",
    "# split of training data into train and validation sets\n",
    "train_size = int(0.8 * len(train_data))\n",
    "val_size = len(train_data) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_data, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WAY = 5  # Number of classes in a task\n",
    "N_SHOT = 5  # Number of images per class in the support set\n",
    "N_QUERY = 10  # Number of images per class in the query set\n",
    "N_EVALUATION_TASKS = 100\n",
    "\n",
    "# The sampler needs a dataset with a \"get_labels\" method. Check the code if you have any doubt!\n",
    "val_subset.get_labels = lambda: [\n",
    "    instance[1] for instance in val_subset\n",
    "]\n",
    "test_data.get_labels = lambda: [\n",
    "    instance[1] for instance in test_data\n",
    "]\n",
    "\n",
    "test_sampler = TaskSampler(\n",
    "    test_data, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS\n",
    ")\n",
    "\n",
    "val_sampler = TaskSampler(\n",
    "    val_subset, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_sampler=test_sampler,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=test_sampler.episodic_collate_fn,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_subset,\n",
    "    batch_sampler=val_sampler,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=val_sampler.episodic_collate_fn,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "N_TASKS_PER_EPOCH = 500\n",
    "N_VALIDATION_TASKS = 100\n",
    "\n",
    "train_data.get_labels = lambda: [instance[1] for instance in train_data]\n",
    "\n",
    "train_sampler = TaskSampler(\n",
    "    train_data, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_TASKS_PER_EPOCH\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_sampler=train_sampler,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=train_sampler.episodic_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Protonets\n",
    "\n",
    "backbone = alexnet(pretrained = True)\n",
    "backbone.classifier[6] = nn.Flatten()\n",
    "model = PrototypicalNetworks(backbone, head = 'ProtoNet').to(device)\n",
    "\n",
    "\n",
    "train_optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "n_epochs = 40\n",
    "\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "best_state = model.state_dict()\n",
    "best_validation_accuracy = 0.0\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    average_loss = training_epoch(model, train_loader, train_optimizer)\n",
    "    validation_accuracy = val_evaluate(model, val_loader)\n",
    "\n",
    "    if validation_accuracy > best_validation_accuracy:\n",
    "        best_validation_accuracy = validation_accuracy\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        # state_dict() returns a reference to the still evolving model's state so we deepcopy\n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models\n",
    "        print(f\"Ding ding ding! We found a new best model! {best_validation_accuracy}\")\n",
    "\n",
    "    # tb_writer.add_scalar(\"Train/loss\", average_loss, epoch)\n",
    "    # tb_writer.add_scalar(\"Val/acc\", validation_accuracy, epoch)\n",
    "\n",
    "    # Warn the scheduler that we did an epoch\n",
    "    # so it knows when to decrease the learning rate\n",
    "    # train_scheduler.step()\n",
    "    train_losses.append(average_loss)\n",
    "    val_accs.append(validation_accuracy)\n",
    "\n",
    "\n",
    "torch.save(best_state, 'cirfar10_protonets.pth')\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_accs)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nate/anaconda3/envs/flags3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/nate/anaconda3/envs/flags3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Eval Subspace Nets\n",
    "\n",
    "backbone = alexnet(pretrained = True)\n",
    "backbone.classifier[6] = nn.Flatten()\n",
    "model = PrototypicalNetworks(backbone, head = 'SubspaceNet')\n",
    "\n",
    "\n",
    "train_optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "n_epochs = 40\n",
    "\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "best_state = model.state_dict()\n",
    "best_validation_accuracy = 0.0\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    average_loss = training_epoch(model, train_loader, train_optimizer)\n",
    "    validation_accuracy = val_evaluate(model, val_loader)\n",
    "\n",
    "    if validation_accuracy > best_validation_accuracy:\n",
    "        best_validation_accuracy = validation_accuracy\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        # state_dict() returns a reference to the still evolving model's state so we deepcopy\n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models\n",
    "        print(f\"Ding ding ding! We found a new best model! {best_validation_accuracy}\")\n",
    "\n",
    "    # tb_writer.add_scalar(\"Train/loss\", average_loss, epoch)\n",
    "    # tb_writer.add_scalar(\"Val/acc\", validation_accuracy, epoch)\n",
    "\n",
    "\n",
    "    train_losses.append(average_loss)\n",
    "    val_accs.append(validation_accuracy)\n",
    "\n",
    "\n",
    "torch.save(best_state, '../models/cirfar10_subspacenets.pth')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_accs)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Eval Subspace Nets\n",
    "\n",
    "my_alexnet = alexnet(pretrained = True)\n",
    "backbone = AlexNetLastTwoLayers(my_alexnet)\n",
    "model = PrototypicalFlagNetworks(backbone)\n",
    "\n",
    "\n",
    "train_optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "n_epochs = 40\n",
    "\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "best_state = model.state_dict()\n",
    "best_validation_accuracy = 0.0\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    average_loss = training_epoch(model, train_loader, train_optimizer)\n",
    "    validation_accuracy = val_evaluate(model, val_loader)\n",
    "\n",
    "    if validation_accuracy > best_validation_accuracy:\n",
    "        best_validation_accuracy = validation_accuracy\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        # state_dict() returns a reference to the still evolving model's state so we deepcopy\n",
    "        # https://pytorch.org/tutorials/beginner/saving_loading_models\n",
    "        print(f\"Ding ding ding! We found a new best model! {best_validation_accuracy}\")\n",
    "\n",
    "    # tb_writer.add_scalar(\"Train/loss\", average_loss, epoch)\n",
    "    # tb_writer.add_scalar(\"Val/acc\", validation_accuracy, epoch)\n",
    "\n",
    "    train_losses.append(average_loss)\n",
    "    val_accs.append(validation_accuracy)\n",
    "    \n",
    "\n",
    "\n",
    "torch.save(best_state, '../models/cifar10_flagnets.pth')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_accs)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "my_first_few_shot_classifier.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "flags3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
